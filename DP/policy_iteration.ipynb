{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"policy_iteration.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNpjEtyq4dr8joPzqfHAkPn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import random\n","from copy import deepcopy as dc\n","\n","\"\"\"\n","Env\n","--------\n","Attribute\n","width, height, reward, possible_actions, all_state\n","--------\n","--------\n","Method\n","get_reward, state_after_action, get_transition_prob, get_all_states\n","--------\n","\"\"\"\n","\n","class PolicyIteration:\n","  def __init__(self, env):\n","    self.env = env\n","    self.value_table = [[0.0] * env.width for _ in range(env.height)]\n","    self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n","                                for _ in range(env.height)]\n","    self.policy_table[2][2] = []\n","    self.discount_factor = 0.9\n","    \n","  def policy_evaluation(self):\n","    '''\n","    update value table with Bellman Expectation Equation\n","    '''\n","    temp_value_table = [[0.0] * self.env.width for _ in range(self.env.height)]\n","    for state in self.env.get_all_states():\n","      \n","      if state == [2, 2]:\n","        continue\n","\n","      new_value = 0.0\n","      # Bellman Expectation Eq\n","      for action in self.env.possible_actions:\n","        next_state = self.env.state_after_action(state, action)\n","        reward = self.env.get_reward(state, action)\n","        next_value = self.get_value(next_state)\n","        probs = self.get_policy(state)\n","        new_value += probs[action] * (reward + self.discount_factor * next_value)\n","      \n","      temp_value_table[state[0]][state[1]] = round(new_value, 2)\n","    \n","    self.value_table = temp_value_table\n","\n","    return\n","\n","  def policy_improvement(self):\n","    temp_policy_table = dc(self.policy_table)\n","\n","    for state in self.env.get_all_states():\n","      if state == [2, 2]:\n","        continue\n","      \n","      value = float(\"-inf\")\n","      high_value_actions = []\n","      for idx, action in enumerate(self.env.possible_actions):\n","        next_state = self.env.state_after_action(state, action)\n","        reward = self.env.get_reward(state, action)\n","        next_value = self.get_value(next_state)\n","        probs = self.get_policy(state)\n","        new_value = probs[action] * (reward + self.discount_factor * next_value)\n","\n","        if new_value > value:\n","          high_value_actions.clear()\n","          high_value_actions.append(idx)\n","          value = new_value\n","        elif new_value == value:\n","          high_value_actions.append(idx)\n","      \n","      temp_prob = [0.0, 0.0, 0.0, 0.0]\n","      prob = 1 / len(high_value_actions)\n","      for idx in high_value_actions:\n","        temp_prob[idx] = prob\n","      \n","      temp_policy_table[state[0]][state[1]] = temp_prob\n","    \n","    self.policy_table = temp_policy_table\n","    return\n","  \n","  def get_value(self, state):\n","    return round(self.value_table[state[0]][state[1]], 2)\n","  \n","  def get_policy(self, state):\n","    return self.policy_table[state[0]][state[1]]\n","  \n","  def get_action(self, state):\n","    random_num = random.randrange(100) / 100\n","    actions = self.get_policy(state)\n","\n","    temp_sum = 0.0\n","    for idx, prob in enumerate(actions):\n","      temp_sum += prob\n","      if random_num < temp_sum:\n","        return idx"],"metadata":{"id":"bcbd_CDPzdgM"},"execution_count":null,"outputs":[]}]}